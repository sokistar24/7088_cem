{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "438342e4-dc08-42dc-80b7-38a856eccf84",
   "metadata": {},
   "source": [
    "# Lesson Outline\n",
    "- History of DQN \n",
    "- Tabular Q learning\n",
    "- The Intuition behind DQN\n",
    "- Fundamental maths \n",
    "- Q Network and Target Network\n",
    "- Experience replay \n",
    "- Fixed Q Target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9e81ed-6892-4e04-888f-a3cdd9be3e39",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/Basic_reinforcement_learning_cycle.png\" alt=\"Reinfocement_learning cycle\" title=\"Reinforcement learning\" width=\"600\" height=\"400\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2dfd9ca-3dc9-403e-ab9a-5039654e49cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall the bellman equation from the tabular Q-learning used to obtain the value for a given state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b337c8-513c-427c-8d11-fe427186550d",
   "metadata": {},
   "source": [
    "# Limitations of Tabular Q-learning\n",
    "\n",
    "- As the environment gets more complex, it becomes infeasible to map every state\n",
    "- Tabular Q-learning would require the agent to visit every state multiple times, which may not be practical or possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9850f4-55b8-4add-b93e-b5c2ad74fe01",
   "metadata": {},
   "source": [
    "# Why Neural Networks\n",
    "\n",
    "- Neural networks estimates values or polices for states the agent has not visited\n",
    "- Fundamental assumtion is that states with similar observations have similar values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c606643-8003-47ca-9e67-91bcb002664b",
   "metadata": {},
   "source": [
    "# Value Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87457bf-ea1a-4a28-97f1-c00d59dfca43",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/value_neural_network.png\" alt=\"Reinfocement_learning cycle\" title=\"Reinforcement learning\" width=\"600\" height=\"400\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a5dd40-2809-4550-a042-27372da0f35a",
   "metadata": {},
   "source": [
    "# Policy Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f9b4cd-a755-406f-b576-4f7861ab4015",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/policy_neural_network.png\" alt=\"Reinfocement_learning cycle\" title=\"Reinforcement learning\" width=\"600\" height=\"400\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58a795c-1696-4485-ad96-102adb37178b",
   "metadata": {},
   "source": [
    "#  Two seperate Neural Networks\n",
    "- Value Neural Network estimates the value at a given state\n",
    "- Policy Neural Network estimates policy at a given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece7f3e-1f3a-4db3-b429-7ece96cb8d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some key questions:\n",
    "# What is the relationship between agent feedback and the networks?\n",
    "# How do we update the weights of these neural networks?\n",
    "# How do we connect the value network with the policy network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741ae3eb-faa2-4d20-872f-e829cdfd1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Insight : Is it possible to frame a reinforcement learning process as a supervised learning process for the neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132a79a4-322d-49ff-afd5-110455a83854",
   "metadata": {},
   "source": [
    "$$\n",
    "V(s) = \\max_a \\left( R(s,a) + \\gamma  V(s') \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7542fa8b-b17b-4755-b747-b79ac838a0c5",
   "metadata": {},
   "source": [
    "# Updating the value Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78549bd4-8c22-4e5b-830f-f7eab351e4e5",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/updating_value_function.png\" alt=\"Updating Value Function\" title=\"Updating Value function\" width=\"600\" height=\"400\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f8516b-52ce-467c-a93c-cf1880cdb446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming a negative reward for each point in the grid, gamma= 0.9 \n",
    "# Using the bellman equation we can have the updated V as V(s) = -1 + 0.9x6 = 4.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3402e3-cbc0-4039-9b8b-cd59e438afeb",
   "metadata": {},
   "source": [
    "**Error Metric:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd889f34-373c-4e4b-bcd2-133251e6973f",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\left( V(s) - \\max_a \\left( R(s,a) + \\gamma \\times V(s') \\right) \\right)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d129b2aa-0f7a-44ce-9eb2-4d17a653839e",
   "metadata": {},
   "source": [
    "- Bellman Equation is used to create the label \n",
    "- The observations are uses as the features \n",
    "- More like a supervised learning problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504b3df0-b579-4c2a-b0d0-0e60897e7f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SO far we have a value network capable of estimating values given observations , but what about the policy \n",
    "# We still need to develop the policy neural network that will choose an action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ed5ab-c56a-44da-b0f7-52d439c7f3a2",
   "metadata": {},
   "source": [
    "# Updating Policy network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4310d2fc-9b3c-49cf-9f82-e5b43bdd2a3f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/updating_policy_network.png\" alt=\"Updating Policy network \" title=\"Updating policy network\" width=\"600\" height=\"400\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d28dd5-ebae-4322-9214-2396e738527a",
   "metadata": {},
   "source": [
    "- What we aim to achieve is increase the probability of taken the right action in the given state\n",
    "- The higher that gain is we want to increase that probability of taking that action in a given state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcbe4db-2a73-4880-a381-91607080f201",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/updating_gain_table.png\" alt=\"Updating gain table \" title=\"Updating gain table\" width=\"600\" height=\"400\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e5d28fb-8c8c-4834-bdbd-a4b5a84c1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming that the probabolity of moving right with the given observation is slow how do we update this to boast the probability\n",
    "# Gradient increase 3 x In(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f93c7cd-90c1-47c7-a451-71dcf98da11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Ideas Remaining \n",
    "# Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2ebd97-4231-428e-8160-c6594619e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A key part of improving performance with DQN is the use of expereince replay\n",
    "# Expereince replay was introduced by Long-Ji Lin in the paepr : \"Self - improving Reactive Agents Based On Reinforcement learning ,Planning and Teaching\" in 1992"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428a527a-5e4a-48a1-b267-b4bade8dfeb2",
   "metadata": {},
   "source": [
    "# Replay Buffer \n",
    "- Recall when using neural networks for supervised learning tasks, we had all the historical data at once\n",
    "- this means we could easily shuffle the data and split into a training set and test set\n",
    "- Randomly shuffling data helped avoid overfitting and the model could more easily generalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4c5a8f-b12c-4906-bada-418db3b3bf83",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/DQN_learning_circle.png\" alt=\"DQN learning circle \" title=\"DQN learning circle\" width=\"800\" height=\"600\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d914d66-797e-4b57-98b3-581f8319dd33",
   "metadata": {},
   "source": [
    "# Difference between the supervised and reinforcment learning problem\n",
    "- Data obtained from the basic reinforcment learning process is gather \"online\". \n",
    "- Online mainly has to do with time series data \n",
    "- Optimisation methods for supervised learning algorithms (such as ANN) often assume data is Independently and Identically Distributed (IID)\n",
    "- For example, in a data set of cat and dog images of different cats are independent of each other \n",
    "- for an ongoing process the experiences of the agent are non IID\n",
    "- The replay buffer helps us to use optimization algorithms developed for IID data to non  - IID data\n",
    "- Another difficulty with most reinforcement learning problems is that the target is changing (i.e the optimal action at a given state at a given time would be different \n",
    "- essentially we have non-stationary targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a617c35f-5497-4f50-b7d3-8b5289f1f76e",
   "metadata": {},
   "source": [
    "# How do we solve the above challenges \n",
    "- Use an expereince replay buffer to sample agent feedback experience to mimic a dataset that is closer to IID\n",
    "- Use a separate target neural network that is updated periodically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f2804b-64f6-4d5b-8c1e-a8995847c809",
   "metadata": {},
   "source": [
    "# Experience Replay Buffer \n",
    "- The buffer holds N steps of agent feedback from the environment (experience)\n",
    "- As the complexity of the environment increases the higher the buffer size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64df9b76-1e45-4f85-8904-55b3427c0103",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/Experience_Replay_Buffer_Deque.png\" alt=\"Expereince Replay \" title=\"Experience\" width=\"800\" height=\"600\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d59b523-f33d-4492-b4f3-485e9380235a",
   "metadata": {},
   "source": [
    "# Mini - Batch\n",
    "- We use the minibatch to obtain random sample from the Replay Buffer\n",
    "- The randmom sampling lower the variance as random samples would be less correlated\n",
    "- This helps improve the robustness of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc418cf-04f4-4403-93b4-5c3bfb4fc670",
   "metadata": {},
   "source": [
    "# Q- Network Model \n",
    "- Goal is to create a neural network that can directly output Q values.\n",
    "- We also want to create a stable target to  train, rather than a target constantly changing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7009b0-8f95-4607-8be6-84b2d3e2214f",
   "metadata": {},
   "source": [
    "# Changing Network Structure \n",
    "- Instead of a policy network directly outputting an action, we would like a Q value instead with Q(s,a)\n",
    "- This allows to apply the epsilon greedy action strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2c618d-031e-4092-afbf-10aa94acc56b",
   "metadata": {},
   "source": [
    " <div style=\"text-align: center;\">\n",
    "  <img src=\"images/Q_network_training_process.png\" alt=\"Q_network Update \" title=\"Q Network\" width=\"800\" height=\"600\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9d884f-f686-4cb9-9446-bf98758cbe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Error = optimal Q(s,a) - Q (S,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c36a418-4d82-4de9-a2fe-b2d00f805c7d",
   "metadata": {},
   "source": [
    "- Obtaining the error we use the experience to get the optimal action at the future time step \n",
    "- However for this to happen with a single network we would have to update the weight twice once for the obtaining the Q value, and second for obtaining the optimal Q\n",
    "- Major issues we would have to do a forward pass twice without backpropagation \n",
    "- Because each pass the weight changes the double pass changes q* \n",
    "- This leads to a very unstable training process, since your target Q is constantly changing with each pass\n",
    "- often this issue is refereed to as chasing your own tail since Q is constantly changing Q *."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2278234-4dcf-49d2-896d-46fcede34712",
   "metadata": {},
   "source": [
    "# How do we solve the changing target\n",
    "- The use of target network\n",
    "- The target network is devoted to the Target Q so is not constantly changing \n",
    "- We can think of the Q- network as the policy network and our Target Network as the value network\n",
    "- We create two identical network\n",
    "- but we only update the target network periodically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6d2c10-894a-430a-8634-fa91b4bdc5ee",
   "metadata": {},
   "source": [
    " <div style=\"text-align: center;\">\n",
    "  <img src=\"images/Target_Network_and_Q_network.png\" alt=\"Q and Target network\" title=\"Q targe\" width=\"800\" height=\"600\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2064aa39-bf93-40cc-9e91-065a87bb8723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
